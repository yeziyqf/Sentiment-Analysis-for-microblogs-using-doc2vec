{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-12-13 13:25:02,945 : INFO : running c:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py -f C:\\Users\\tianx\\AppData\\Roaming\\jupyter\\runtime\\kernel-cb96e0aa-82f2-459e-8d90-9a9e7b9aa454.json\n",
      "2017-12-13 13:25:03,114 : INFO : collecting all words and their counts\n",
      "2017-12-13 13:25:03,130 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-12-13 13:25:03,197 : INFO : collected 2998 word types and 1636 unique tags from a corpus of 1636 examples and 15172 words\n",
      "2017-12-13 13:25:03,210 : INFO : Loading a fresh vocabulary\n",
      "2017-12-13 13:25:03,249 : INFO : min_count=1 retains 2998 unique words (100% of original 2998, drops 0)\n",
      "2017-12-13 13:25:03,256 : INFO : min_count=1 leaves 15172 word corpus (100% of original 15172, drops 0)\n",
      "2017-12-13 13:25:03,309 : INFO : deleting the raw counts dictionary of 2998 items\n",
      "2017-12-13 13:25:03,336 : INFO : sample=0.0001 downsamples 809 most-common words\n",
      "2017-12-13 13:25:03,345 : INFO : downsampling leaves estimated 7990 word corpus (52.7% of prior 15172)\n",
      "2017-12-13 13:25:03,360 : INFO : estimated required memory for 2998 words and 100 dimensions: 4879000 bytes\n",
      "2017-12-13 13:25:03,405 : INFO : resetting layer weights\n",
      "2017-12-13 13:25:03,750 : INFO : Epoch 0\n",
      "2017-12-13 13:25:03,776 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:04,615 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:04,624 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:04,647 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:04,665 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:04,682 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:04,732 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:04,762 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:04,769 : INFO : training on 75860 raw words (48021 effective words) took 0.9s, 51330 effective words/s\n",
      "2017-12-13 13:25:04,773 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:04,776 : INFO : Epoch 1\n",
      "2017-12-13 13:25:04,785 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:05,702 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:05,717 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:05,743 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:05,754 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:05,766 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:05,775 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:05,807 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:05,813 : INFO : training on 75860 raw words (48159 effective words) took 0.9s, 53524 effective words/s\n",
      "2017-12-13 13:25:05,819 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:05,825 : INFO : Epoch 2\n",
      "2017-12-13 13:25:05,836 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:06,585 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:06,614 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:06,620 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:06,630 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:06,643 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:06,650 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:06,701 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:06,709 : INFO : training on 75860 raw words (48044 effective words) took 0.8s, 57632 effective words/s\n",
      "2017-12-13 13:25:06,714 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:06,720 : INFO : Epoch 3\n",
      "2017-12-13 13:25:06,731 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:07,404 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:07,410 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:07,420 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:07,425 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:07,433 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:07,441 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:07,493 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:07,499 : INFO : training on 75860 raw words (48186 effective words) took 0.7s, 65037 effective words/s\n",
      "2017-12-13 13:25:07,507 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:07,512 : INFO : Epoch 4\n",
      "2017-12-13 13:25:07,524 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:08,231 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:08,244 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:08,249 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:08,259 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:08,271 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:08,280 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:08,317 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:08,321 : INFO : training on 75860 raw words (48140 effective words) took 0.8s, 62077 effective words/s\n",
      "2017-12-13 13:25:08,325 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:08,328 : INFO : Epoch 5\n",
      "2017-12-13 13:25:08,337 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:09,037 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:09,082 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:09,088 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:09,124 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:09,140 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:09,151 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:09,184 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:09,192 : INFO : training on 75860 raw words (48133 effective words) took 0.8s, 57468 effective words/s\n",
      "2017-12-13 13:25:09,199 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:09,215 : INFO : Epoch 6\n",
      "2017-12-13 13:25:09,232 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:09,931 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:09,948 : INFO : worker thread finished; awaiting finish of 5 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-13 13:25:09,956 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:09,970 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:09,984 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:09,991 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:10,036 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:10,039 : INFO : training on 75860 raw words (48274 effective words) took 0.8s, 61430 effective words/s\n",
      "2017-12-13 13:25:10,044 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:10,053 : INFO : Epoch 7\n",
      "2017-12-13 13:25:10,064 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:10,729 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:10,744 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:10,755 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:10,771 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:10,778 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:10,786 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:10,826 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:10,832 : INFO : training on 75860 raw words (47974 effective words) took 0.7s, 65234 effective words/s\n",
      "2017-12-13 13:25:10,836 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:10,840 : INFO : Epoch 8\n",
      "2017-12-13 13:25:10,851 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:11,567 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:11,579 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:11,598 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:11,613 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:11,627 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:11,640 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:11,673 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:11,681 : INFO : training on 75860 raw words (48237 effective words) took 0.8s, 59646 effective words/s\n",
      "2017-12-13 13:25:11,688 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:11,692 : INFO : Epoch 9\n",
      "2017-12-13 13:25:11,700 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:12,359 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:12,383 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:12,408 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:12,418 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:12,430 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:12,440 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:12,479 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:12,490 : INFO : training on 75860 raw words (48114 effective words) took 0.8s, 62467 effective words/s\n",
      "2017-12-13 13:25:12,497 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:12,506 : INFO : Epoch 10\n",
      "2017-12-13 13:25:12,517 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:13,247 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:13,270 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:13,281 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:13,307 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:13,319 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:13,329 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:13,369 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:13,376 : INFO : training on 75860 raw words (48073 effective words) took 0.8s, 57534 effective words/s\n",
      "2017-12-13 13:25:13,379 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:13,389 : INFO : Epoch 11\n",
      "2017-12-13 13:25:13,404 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:14,087 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:14,096 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:14,107 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:14,117 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:14,126 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:14,136 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:14,191 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:14,196 : INFO : training on 75860 raw words (48010 effective words) took 0.8s, 62211 effective words/s\n",
      "2017-12-13 13:25:14,201 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:14,207 : INFO : Epoch 12\n",
      "2017-12-13 13:25:14,219 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:14,997 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:15,021 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:15,031 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:15,046 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:15,054 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:15,063 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:15,104 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:15,112 : INFO : training on 75860 raw words (48089 effective words) took 0.9s, 55102 effective words/s\n",
      "2017-12-13 13:25:15,118 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:15,127 : INFO : Epoch 13\n",
      "2017-12-13 13:25:15,135 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:15,815 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:15,855 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:15,860 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:15,866 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:15,876 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:15,883 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:15,928 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:15,934 : INFO : training on 75860 raw words (48328 effective words) took 0.8s, 62501 effective words/s\n",
      "2017-12-13 13:25:15,937 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-13 13:25:15,944 : INFO : Epoch 14\n",
      "2017-12-13 13:25:15,954 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:16,697 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:16,719 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:16,735 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:16,747 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:16,759 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:16,780 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:16,902 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:16,912 : INFO : training on 75860 raw words (48053 effective words) took 0.9s, 51317 effective words/s\n",
      "2017-12-13 13:25:16,916 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:16,921 : INFO : Epoch 15\n",
      "2017-12-13 13:25:16,935 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:17,719 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:17,727 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:17,742 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:17,754 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:17,765 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:17,775 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:17,823 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:17,830 : INFO : training on 75860 raw words (48154 effective words) took 0.8s, 59093 effective words/s\n",
      "2017-12-13 13:25:17,836 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:17,843 : INFO : Epoch 16\n",
      "2017-12-13 13:25:17,854 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:18,554 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:18,570 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:18,615 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:18,633 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:18,640 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:18,647 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:18,685 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:18,690 : INFO : training on 75860 raw words (47962 effective words) took 0.8s, 58525 effective words/s\n",
      "2017-12-13 13:25:18,694 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:18,701 : INFO : Epoch 17\n",
      "2017-12-13 13:25:18,708 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:19,400 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:19,502 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:19,567 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:19,574 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:19,579 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:19,590 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:19,624 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:19,628 : INFO : training on 75860 raw words (48136 effective words) took 0.9s, 53714 effective words/s\n",
      "2017-12-13 13:25:19,633 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:19,639 : INFO : Epoch 18\n",
      "2017-12-13 13:25:19,665 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:20,397 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:20,415 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:20,425 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:20,433 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:20,452 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:20,462 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:20,506 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:20,517 : INFO : training on 75860 raw words (48111 effective words) took 0.8s, 61400 effective words/s\n",
      "2017-12-13 13:25:20,523 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:20,531 : INFO : Epoch 19\n",
      "2017-12-13 13:25:20,548 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:21,374 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:21,403 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:21,415 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:21,428 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:21,445 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:21,449 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:21,523 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:21,534 : INFO : training on 75860 raw words (48203 effective words) took 0.9s, 51093 effective words/s\n",
      "2017-12-13 13:25:21,545 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:21,548 : INFO : Epoch 20\n",
      "2017-12-13 13:25:21,557 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:22,372 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:22,406 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:22,426 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:22,437 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:22,457 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:22,465 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:22,515 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:22,523 : INFO : training on 75860 raw words (48158 effective words) took 0.9s, 51600 effective words/s\n",
      "2017-12-13 13:25:22,529 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:22,533 : INFO : Epoch 21\n",
      "2017-12-13 13:25:22,545 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:23,330 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:23,387 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:23,400 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:23,416 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:23,431 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-13 13:25:23,440 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:23,497 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:23,503 : INFO : training on 75860 raw words (48121 effective words) took 0.9s, 52887 effective words/s\n",
      "2017-12-13 13:25:23,507 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:23,513 : INFO : Epoch 22\n",
      "2017-12-13 13:25:23,523 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:24,203 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:24,209 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:24,220 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:24,240 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:24,251 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:24,263 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:24,342 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:24,348 : INFO : training on 75860 raw words (48166 effective words) took 0.8s, 60828 effective words/s\n",
      "2017-12-13 13:25:24,351 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:24,360 : INFO : Epoch 23\n",
      "2017-12-13 13:25:24,373 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:25,164 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:25,176 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:25,224 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:25,236 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:25,247 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:25,256 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:25,316 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:25,324 : INFO : training on 75860 raw words (48234 effective words) took 0.9s, 52400 effective words/s\n",
      "2017-12-13 13:25:25,329 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:25,335 : INFO : Epoch 24\n",
      "2017-12-13 13:25:25,347 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:26,217 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:26,226 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:26,235 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:26,246 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:26,266 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:26,283 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:26,335 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:26,344 : INFO : training on 75860 raw words (48136 effective words) took 0.9s, 52757 effective words/s\n",
      "2017-12-13 13:25:26,350 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:26,359 : INFO : Epoch 25\n",
      "2017-12-13 13:25:26,368 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:27,124 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:27,160 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:27,169 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:27,228 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:27,253 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:27,266 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:27,325 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:27,333 : INFO : training on 75860 raw words (48071 effective words) took 0.9s, 51078 effective words/s\n",
      "2017-12-13 13:25:27,342 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:27,349 : INFO : Epoch 26\n",
      "2017-12-13 13:25:27,363 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:28,003 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:28,043 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:28,059 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:28,067 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:28,081 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:28,089 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:28,142 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:28,148 : INFO : training on 75860 raw words (48007 effective words) took 0.8s, 62917 effective words/s\n",
      "2017-12-13 13:25:28,154 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:28,158 : INFO : Epoch 27\n",
      "2017-12-13 13:25:28,168 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:28,984 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:28,995 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:29,000 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:29,014 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:29,024 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:29,034 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:29,098 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:29,104 : INFO : training on 75860 raw words (48253 effective words) took 0.9s, 52719 effective words/s\n",
      "2017-12-13 13:25:29,113 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:29,120 : INFO : Epoch 28\n",
      "2017-12-13 13:25:29,129 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:29,872 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:29,882 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:29,891 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:29,903 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:29,912 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:29,918 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:29,993 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:30,001 : INFO : training on 75860 raw words (48164 effective words) took 0.9s, 56525 effective words/s\n",
      "2017-12-13 13:25:30,004 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:30,009 : INFO : Epoch 29\n",
      "2017-12-13 13:25:30,019 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:30,695 : INFO : worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-13 13:25:30,703 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:30,708 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:30,721 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:30,731 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:30,738 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:30,781 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:30,785 : INFO : training on 75860 raw words (48319 effective words) took 0.8s, 64260 effective words/s\n",
      "2017-12-13 13:25:30,790 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:30,798 : INFO : Epoch 30\n",
      "2017-12-13 13:25:30,806 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:31,484 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:31,499 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:31,508 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:31,514 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:31,525 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:31,533 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:31,573 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:31,580 : INFO : training on 75860 raw words (47920 effective words) took 0.8s, 63622 effective words/s\n",
      "2017-12-13 13:25:31,584 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:31,588 : INFO : Epoch 31\n",
      "2017-12-13 13:25:31,597 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:32,285 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:32,298 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:32,306 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:32,320 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:32,333 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:32,356 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:32,419 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:32,425 : INFO : training on 75860 raw words (48053 effective words) took 0.8s, 59129 effective words/s\n",
      "2017-12-13 13:25:32,430 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:32,440 : INFO : Epoch 32\n",
      "2017-12-13 13:25:32,457 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:33,215 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:33,224 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:33,255 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:33,267 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:33,281 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:33,287 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:33,345 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:33,354 : INFO : training on 75860 raw words (48216 effective words) took 0.9s, 55505 effective words/s\n",
      "2017-12-13 13:25:33,358 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:33,363 : INFO : Epoch 33\n",
      "2017-12-13 13:25:33,373 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:34,098 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:34,110 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:34,126 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:34,140 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:34,150 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:34,167 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:34,215 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:34,224 : INFO : training on 75860 raw words (48216 effective words) took 0.8s, 58346 effective words/s\n",
      "2017-12-13 13:25:34,227 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:34,232 : INFO : Epoch 34\n",
      "2017-12-13 13:25:34,241 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:35,048 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:35,094 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:35,103 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:35,118 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:35,143 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:35,152 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:35,199 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:35,207 : INFO : training on 75860 raw words (48058 effective words) took 0.9s, 51088 effective words/s\n",
      "2017-12-13 13:25:35,216 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:35,225 : INFO : Epoch 35\n",
      "2017-12-13 13:25:35,238 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:36,139 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:36,145 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:36,154 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:36,165 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:36,174 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:36,184 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:36,247 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:36,253 : INFO : training on 75860 raw words (48090 effective words) took 1.0s, 48336 effective words/s\n",
      "2017-12-13 13:25:36,256 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:36,262 : INFO : Epoch 36\n",
      "2017-12-13 13:25:36,270 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:36,969 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:36,980 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:36,994 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:37,011 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:37,087 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:37,098 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:37,147 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:37,155 : INFO : training on 75860 raw words (48076 effective words) took 0.9s, 56428 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-13 13:25:37,163 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:37,171 : INFO : Epoch 37\n",
      "2017-12-13 13:25:37,179 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:37,939 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:37,992 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:38,015 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:38,031 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:38,038 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:38,053 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:38,129 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:38,138 : INFO : training on 75860 raw words (48244 effective words) took 0.9s, 52880 effective words/s\n",
      "2017-12-13 13:25:38,151 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:38,160 : INFO : Epoch 38\n",
      "2017-12-13 13:25:38,168 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:38,945 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:38,979 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:39,042 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:39,055 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:39,068 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:39,079 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:39,110 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:39,119 : INFO : training on 75860 raw words (48000 effective words) took 0.9s, 52566 effective words/s\n",
      "2017-12-13 13:25:39,132 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:39,146 : INFO : Epoch 39\n",
      "2017-12-13 13:25:39,158 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:39,976 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:39,991 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:40,022 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:40,029 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:40,043 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:40,053 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:40,167 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:40,180 : INFO : training on 75860 raw words (48166 effective words) took 0.9s, 51448 effective words/s\n",
      "2017-12-13 13:25:40,193 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:40,211 : INFO : Epoch 40\n",
      "2017-12-13 13:25:40,224 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:41,140 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:41,196 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:41,212 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:41,235 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:41,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:41,264 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:41,447 : INFO : PROGRESS: at 100.00% examples, 42808 words/s, in_qsize 0, out_qsize 1\n",
      "2017-12-13 13:25:41,460 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:41,466 : INFO : training on 75860 raw words (48165 effective words) took 1.1s, 42091 effective words/s\n",
      "2017-12-13 13:25:41,475 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:41,483 : INFO : Epoch 41\n",
      "2017-12-13 13:25:41,496 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:42,853 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:42,860 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:42,876 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:42,891 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:42,900 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:42,907 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:43,021 : INFO : PROGRESS: at 100.00% examples, 46243 words/s, in_qsize 0, out_qsize 1\n",
      "2017-12-13 13:25:43,039 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:43,047 : INFO : training on 75860 raw words (48018 effective words) took 1.1s, 45102 effective words/s\n",
      "2017-12-13 13:25:43,056 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:43,061 : INFO : Epoch 42\n",
      "2017-12-13 13:25:43,073 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:43,932 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:43,965 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:43,985 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:43,996 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:44,009 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:44,021 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:44,094 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:44,115 : INFO : training on 75860 raw words (48112 effective words) took 1.0s, 47903 effective words/s\n",
      "2017-12-13 13:25:44,120 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:44,129 : INFO : Epoch 43\n",
      "2017-12-13 13:25:44,140 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:45,186 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:45,197 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:45,208 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:45,220 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:45,228 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:45,241 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:45,329 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:45,336 : INFO : training on 75860 raw words (47970 effective words) took 0.9s, 53149 effective words/s\n",
      "2017-12-13 13:25:45,339 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:45,343 : INFO : Epoch 44\n",
      "2017-12-13 13:25:45,352 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:46,280 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:46,326 : INFO : worker thread finished; awaiting finish of 5 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-13 13:25:46,337 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:46,345 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:46,358 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:46,378 : INFO : PROGRESS: at 92.09% examples, 43949 words/s, in_qsize 1, out_qsize 1\n",
      "2017-12-13 13:25:46,396 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:46,449 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:46,456 : INFO : training on 75860 raw words (47933 effective words) took 1.1s, 44227 effective words/s\n",
      "2017-12-13 13:25:46,458 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:46,461 : INFO : Epoch 45\n",
      "2017-12-13 13:25:46,468 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:47,282 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:47,329 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:47,343 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:47,366 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:47,376 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:47,381 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:47,505 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:47,512 : INFO : training on 75860 raw words (48052 effective words) took 1.0s, 50194 effective words/s\n",
      "2017-12-13 13:25:47,516 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:47,520 : INFO : Epoch 46\n",
      "2017-12-13 13:25:47,529 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:48,491 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:48,522 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:48,558 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:48,570 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:48,574 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:48,582 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:48,620 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:48,627 : INFO : training on 75860 raw words (48111 effective words) took 0.9s, 55809 effective words/s\n",
      "2017-12-13 13:25:48,632 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:48,636 : INFO : Epoch 47\n",
      "2017-12-13 13:25:48,646 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:49,421 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:49,454 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:49,463 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:49,470 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:49,480 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:49,491 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:49,525 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:49,532 : INFO : training on 75860 raw words (48130 effective words) took 0.9s, 55485 effective words/s\n",
      "2017-12-13 13:25:49,535 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:49,540 : INFO : Epoch 48\n",
      "2017-12-13 13:25:49,549 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:50,224 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:50,231 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:50,270 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:50,292 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:50,301 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:50,307 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:50,408 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:50,415 : INFO : training on 75860 raw words (48138 effective words) took 0.8s, 56850 effective words/s\n",
      "2017-12-13 13:25:50,417 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:50,424 : INFO : Epoch 49\n",
      "2017-12-13 13:25:50,433 : INFO : training model with 7 workers on 2998 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:51,085 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:51,132 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:51,161 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:51,174 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:51,183 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:51,193 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:51,273 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:51,280 : INFO : training on 75860 raw words (48149 effective words) took 0.8s, 58413 effective words/s\n",
      "2017-12-13 13:25:51,287 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:51,291 : INFO : saving Doc2Vec object under ./imdb.d2v, separately None\n",
      "2017-12-13 13:25:51,297 : INFO : not storing attribute syn0norm\n",
      "2017-12-13 13:25:51,304 : INFO : not storing attribute cum_table\n",
      "2017-12-13 13:25:51,769 : INFO : saved ./imdb.d2v\n"
     ]
    }
   ],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# shuffle\n",
    "from random import shuffle\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "import _pickle as pickle\n",
    "#import cPickle as pickle   #Note: in python3, _pickle was used instead of cpickle\n",
    "\n",
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "\n",
    "        flipped = {}\n",
    "\n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "\n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(\n",
    "                        utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "\n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences\n",
    "\n",
    "sources = {'test-neg-headlines.txt':'TEST_NEG', 'test-pos-headlines.txt':'TEST_POS', 'train-neg-headlines.txt':'TRAIN_NEG', 'train-pos-headlines.txt':'TRAIN_POS', 'train-unsup.txt':'TRAIN_UNS'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)\n",
    "\n",
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model.build_vocab(sentences.to_array())\n",
    "\n",
    "for epoch in range(50):\n",
    "    logger.info('Epoch %d' % epoch)\n",
    "    model.train(sentences.sentences_perm(),\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter,\n",
    "    )\n",
    "\n",
    "model.save('./imdb.d2v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-13 16:53:32,372 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('label', 0.9580812454223633),\n",
       " ('azd', 0.9504972696304321),\n",
       " ('selumetinib', 0.9473737478256226),\n",
       " ('orphan', 0.9285398721694946),\n",
       " ('status', 0.9002965688705444),\n",
       " ('immune', 0.8734671473503113),\n",
       " ('cancer', 0.8715850114822388),\n",
       " ('acerta', 0.8587213754653931),\n",
       " ('drug', 0.8533174991607666),\n",
       " ('divested', 0.8283589482307434)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('death', 0.8398493528366089),\n",
       " ('suggests', 0.824703574180603),\n",
       " ('onglyza', 0.750079870223999),\n",
       " ('aims', 0.6878533363342285),\n",
       " ('diabetes', 0.6724579334259033),\n",
       " ('data', 0.633600115776062),\n",
       " ('rate', 0.6235688328742981),\n",
       " ('fda', 0.6177887916564941),\n",
       " ('azd', 0.6037763357162476),\n",
       " ('ratio', 0.599388599395752)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('increase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((1142,100))\n",
    "train_labels = numpy.zeros((1142))\n",
    "for i in range(691):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    train_arrays[i] = model.docvecs[prefix_train_pos]\n",
    "    train_labels[i] = 1\n",
    "for i in range(451):\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays[i+451] = model.docvecs[prefix_train_neg]\n",
    "    train_labels[i+451] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03945563, -0.36040515, -0.81756526, -0.24152143, -0.10632309,\n",
       "       -0.16919744, -0.25610462, -0.49840897, -0.31717134,  0.39454591,\n",
       "        0.14760627,  0.07900509, -0.20074853,  0.11773267, -0.11075872,\n",
       "       -0.09591108, -0.21288222, -0.11662704, -0.09278214,  0.01988182,\n",
       "        0.67003477,  0.44293499, -0.43041474,  0.10573954, -0.06735239,\n",
       "        0.15747884,  0.20386039, -0.11460374, -0.03202608, -0.13767077,\n",
       "       -0.09739126, -0.33748809, -0.26100436, -0.47627538,  0.12671883,\n",
       "       -0.47341105, -0.45879179,  0.4798207 , -0.10793679, -0.16957298,\n",
       "       -0.01653943, -0.06163348, -0.09538887,  0.40632832, -0.11863247,\n",
       "       -0.08789644, -0.34918171, -0.1562492 ,  0.46208948,  0.1314804 ,\n",
       "       -0.02277557,  0.1046523 , -0.11416169, -0.20348941, -0.21107732,\n",
       "        0.11291379,  0.32570395,  0.50531346, -0.20624495, -0.15568005,\n",
       "        0.30965835, -0.13383634, -0.44210556, -0.34768116,  0.5247438 ,\n",
       "       -0.30657843, -0.17650133,  0.07744043, -0.22693506,  0.15453117,\n",
       "       -0.00427318,  0.11153056, -0.09737778, -0.30167857, -0.0952058 ,\n",
       "        0.68363899,  0.20687847,  0.3096894 , -0.17814587,  0.13528189,\n",
       "       -0.16337898, -0.36092752, -0.08015571, -0.26400235, -0.02879228,\n",
       "       -0.19609654,  0.32564169,  0.47448596,  0.09416756,  0.24747594,\n",
       "       -0.00283856, -0.51175946, -0.75998318, -0.00340536,  0.04066262,\n",
       "       -0.4296127 , -0.18150026, -0.36047032, -0.37883219,  0.07806898], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs['TRAIN_POS_690']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1035524   0.32813051 -0.22517553 ..., -0.58696747 -0.46383634\n",
      "  -0.06675959]\n",
      " [-0.50630361 -0.07826949 -0.25960752 ..., -0.03545469 -0.87155759\n",
      "  -0.39803335]\n",
      " [ 0.06695969  0.11612151 -0.42143866 ...,  0.00394759 -0.33607072\n",
      "   0.05373187]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(train_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays = numpy.zeros((491,100))\n",
    "test_labels = numpy.zeros((491))\n",
    "for i in range(288):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    test_arrays[i] = model.docvecs[prefix_test_pos]\n",
    "    test_labels[i] = 1\n",
    "for i in range(203):\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    test_arrays[i+288] = model.docvecs[prefix_test_neg]\n",
    "    test_labels[i+288] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66191446028513234"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=42, splitter='best')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = tree_reg.predict(train_arrays)\n",
    "tree_mse = mean_squared_error(train_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.76666667,  0.74193548,  0.68571429,  0.64705882,  0.7       ,\n",
       "        0.70967742,  0.71875   ,  0.75      ,  0.70588235,  0.64516129])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "precision_list = cross_val_score(classifier, test_arrays, test_labels, cv=10, scoring='precision')\n",
    "precision_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70708463223999285"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "for item in precision_list:\n",
    "    sum += item\n",
    "precision = sum/len(precision_list)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.79310345,  0.79310345,  0.82758621,  0.75862069,  0.72413793,\n",
       "        0.75862069,  0.79310345,  0.62068966,  0.85714286,  0.71428571])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_list = cross_val_score(classifier, test_arrays, test_labels, cv=10, scoring='recall')\n",
    "recall_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76403940886699506"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_recall = 0\n",
    "for item in recall_list:\n",
    "    sum_recall += item\n",
    "recall = sum_recall/len(recall_list)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73445951441193469"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = 2*(precision * recall)/(precision + recall)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
