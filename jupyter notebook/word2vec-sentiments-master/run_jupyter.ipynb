{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-12-13 13:24:57,241 : INFO : running c:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py -f C:\\Users\\tianx\\AppData\\Roaming\\jupyter\\runtime\\kernel-d2a6e91b-b10c-4dea-9b42-8b9f3076776b.json\n",
      "2017-12-13 13:24:57,588 : INFO : collecting all words and their counts\n",
      "2017-12-13 13:24:57,590 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-12-13 13:24:57,637 : INFO : collected 2712 word types and 2497 unique tags from a corpus of 2497 examples and 14434 words\n",
      "2017-12-13 13:24:57,639 : INFO : Loading a fresh vocabulary\n",
      "2017-12-13 13:24:57,661 : INFO : min_count=1 retains 2712 unique words (100% of original 2712, drops 0)\n",
      "2017-12-13 13:24:57,663 : INFO : min_count=1 leaves 14434 word corpus (100% of original 14434, drops 0)\n",
      "2017-12-13 13:24:57,703 : INFO : deleting the raw counts dictionary of 2712 items\n",
      "2017-12-13 13:24:57,706 : INFO : sample=0.0001 downsamples 755 most-common words\n",
      "2017-12-13 13:24:57,714 : INFO : downsampling leaves estimated 6981 word corpus (48.4% of prior 14434)\n",
      "2017-12-13 13:24:57,717 : INFO : estimated required memory for 2712 words and 100 dimensions: 5023800 bytes\n",
      "2017-12-13 13:24:57,749 : INFO : resetting layer weights\n",
      "2017-12-13 13:24:57,928 : INFO : Epoch 0\n",
      "2017-12-13 13:24:57,932 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:24:58,940 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:24:58,949 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:24:58,967 : INFO : PROGRESS: at 55.58% examples, 26158 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-13 13:24:58,968 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:24:58,980 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:24:58,993 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:24:59,012 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:24:59,030 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:24:59,032 : INFO : training on 72170 raw words (47508 effective words) took 1.1s, 44188 effective words/s\n",
      "2017-12-13 13:24:59,038 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:24:59,041 : INFO : Epoch 1\n",
      "2017-12-13 13:24:59,056 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:24:59,961 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:24:59,970 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:24:59,975 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:24:59,981 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:24:59,992 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:00,008 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:00,032 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:00,034 : INFO : training on 72170 raw words (47279 effective words) took 1.0s, 49293 effective words/s\n",
      "2017-12-13 13:25:00,041 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:00,045 : INFO : Epoch 2\n",
      "2017-12-13 13:25:00,057 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:01,114 : INFO : PROGRESS: at 13.69% examples, 6300 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:01,137 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:01,160 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:01,168 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:01,178 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:01,187 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:01,201 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:01,227 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:01,229 : INFO : training on 72170 raw words (47379 effective words) took 1.2s, 40887 effective words/s\n",
      "2017-12-13 13:25:01,235 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:01,238 : INFO : Epoch 3\n",
      "2017-12-13 13:25:01,254 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:02,384 : INFO : PROGRESS: at 13.94% examples, 6133 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:02,418 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:02,431 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:02,439 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:02,443 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:02,451 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:02,459 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:02,480 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:02,482 : INFO : training on 72170 raw words (47501 effective words) took 1.2s, 40702 effective words/s\n",
      "2017-12-13 13:25:02,485 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:02,487 : INFO : Epoch 4\n",
      "2017-12-13 13:25:02,498 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:03,674 : INFO : PROGRESS: at 13.88% examples, 5657 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:03,683 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:03,697 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:03,740 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:03,750 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:03,760 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:03,770 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:03,784 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:03,792 : INFO : training on 72170 raw words (47259 effective words) took 1.3s, 37043 effective words/s\n",
      "2017-12-13 13:25:03,797 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:03,801 : INFO : Epoch 5\n",
      "2017-12-13 13:25:03,812 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:04,835 : INFO : PROGRESS: at 13.79% examples, 6530 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:05,049 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:05,078 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:05,087 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:05,095 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:05,155 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:05,163 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:05,182 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:05,184 : INFO : training on 72170 raw words (47412 effective words) took 1.4s, 35056 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-13 13:25:05,188 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:05,191 : INFO : Epoch 6\n",
      "2017-12-13 13:25:05,205 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:06,363 : INFO : PROGRESS: at 13.84% examples, 6217 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:06,377 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:06,387 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:06,408 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:06,418 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:06,435 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:06,445 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:06,468 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:06,470 : INFO : training on 72170 raw words (47174 effective words) took 1.2s, 40685 effective words/s\n",
      "2017-12-13 13:25:06,476 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:06,479 : INFO : Epoch 7\n",
      "2017-12-13 13:25:06,492 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:07,533 : INFO : PROGRESS: at 13.91% examples, 6393 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:07,541 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:07,549 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:07,567 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:07,577 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:07,591 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:07,597 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:07,621 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:07,623 : INFO : training on 72170 raw words (47316 effective words) took 1.1s, 42457 effective words/s\n",
      "2017-12-13 13:25:07,626 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:07,632 : INFO : Epoch 8\n",
      "2017-12-13 13:25:07,646 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:08,675 : INFO : PROGRESS: at 27.60% examples, 13013 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-13 13:25:08,683 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:08,686 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:08,713 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:08,716 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:08,724 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:08,733 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:08,757 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:08,760 : INFO : training on 72170 raw words (47286 effective words) took 1.1s, 43246 effective words/s\n",
      "2017-12-13 13:25:08,765 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:08,770 : INFO : Epoch 9\n",
      "2017-12-13 13:25:08,781 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:09,800 : INFO : PROGRESS: at 27.60% examples, 13023 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-13 13:25:09,803 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:09,841 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:09,878 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:09,894 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:09,905 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:09,916 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:09,921 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:09,924 : INFO : training on 72170 raw words (47514 effective words) took 1.1s, 42210 effective words/s\n",
      "2017-12-13 13:25:09,929 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:09,932 : INFO : Epoch 10\n",
      "2017-12-13 13:25:09,944 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:10,960 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:10,969 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:10,982 : INFO : PROGRESS: at 55.57% examples, 26281 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-13 13:25:10,984 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:11,005 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:11,015 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:11,033 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:11,054 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:11,057 : INFO : training on 72170 raw words (47548 effective words) took 1.1s, 44077 effective words/s\n",
      "2017-12-13 13:25:11,060 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:11,067 : INFO : Epoch 11\n",
      "2017-12-13 13:25:11,075 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:12,143 : INFO : PROGRESS: at 27.69% examples, 12580 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-13 13:25:12,145 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:12,163 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:12,171 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:12,178 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:12,193 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:12,199 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:12,216 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:12,218 : INFO : training on 72170 raw words (47450 effective words) took 1.1s, 42579 effective words/s\n",
      "2017-12-13 13:25:12,225 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:12,229 : INFO : Epoch 12\n",
      "2017-12-13 13:25:12,237 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:13,282 : INFO : PROGRESS: at 27.53% examples, 12755 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-13 13:25:13,287 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:13,347 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:13,356 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:13,378 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:13,394 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:13,402 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:13,418 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:13,420 : INFO : training on 72170 raw words (47375 effective words) took 1.2s, 40576 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-13 13:25:13,423 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:13,426 : INFO : Epoch 13\n",
      "2017-12-13 13:25:13,436 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:14,451 : INFO : PROGRESS: at 27.51% examples, 13086 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-13 13:25:14,458 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:14,504 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:14,521 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:14,541 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:14,553 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:14,557 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:14,580 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:14,584 : INFO : training on 72170 raw words (47388 effective words) took 1.1s, 41835 effective words/s\n",
      "2017-12-13 13:25:14,593 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:14,596 : INFO : Epoch 14\n",
      "2017-12-13 13:25:14,606 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:15,658 : INFO : PROGRESS: at 27.62% examples, 12935 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-13 13:25:15,662 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:15,704 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:15,718 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:15,726 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:15,738 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:15,749 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:15,761 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:15,763 : INFO : training on 72170 raw words (47425 effective words) took 1.1s, 42357 effective words/s\n",
      "2017-12-13 13:25:15,771 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:15,774 : INFO : Epoch 15\n",
      "2017-12-13 13:25:15,786 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:16,937 : INFO : PROGRESS: at 14.03% examples, 5846 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:16,969 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:16,981 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:16,991 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:17,013 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:17,030 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:17,041 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:17,056 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:17,058 : INFO : training on 72170 raw words (47412 effective words) took 1.3s, 37722 effective words/s\n",
      "2017-12-13 13:25:17,064 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:17,071 : INFO : Epoch 16\n",
      "2017-12-13 13:25:17,086 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:18,136 : INFO : PROGRESS: at 27.55% examples, 12670 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-13 13:25:18,138 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:18,154 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:18,162 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:18,174 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:18,184 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:18,202 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:18,216 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:18,221 : INFO : training on 72170 raw words (47267 effective words) took 1.1s, 42247 effective words/s\n",
      "2017-12-13 13:25:18,224 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:18,228 : INFO : Epoch 17\n",
      "2017-12-13 13:25:18,240 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:19,307 : INFO : PROGRESS: at 27.67% examples, 12573 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-13 13:25:19,310 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:19,337 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:19,349 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:19,386 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:19,399 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:19,409 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:19,418 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:19,420 : INFO : training on 72170 raw words (47419 effective words) took 1.2s, 40877 effective words/s\n",
      "2017-12-13 13:25:19,424 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:19,429 : INFO : Epoch 18\n",
      "2017-12-13 13:25:19,437 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:20,591 : INFO : PROGRESS: at 13.96% examples, 6044 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:20,595 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:20,627 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:20,642 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:20,655 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:20,659 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:20,668 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:20,690 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:20,692 : INFO : training on 72170 raw words (47364 effective words) took 1.2s, 39893 effective words/s\n",
      "2017-12-13 13:25:20,695 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:20,702 : INFO : Epoch 19\n",
      "2017-12-13 13:25:20,716 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:21,914 : INFO : PROGRESS: at 13.94% examples, 5579 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:21,935 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:21,967 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:21,984 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:21,997 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:22,005 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:22,031 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:22,057 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:22,061 : INFO : training on 72170 raw words (47340 effective words) took 1.3s, 35749 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-13 13:25:22,069 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:22,072 : INFO : Epoch 20\n",
      "2017-12-13 13:25:22,085 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:23,215 : INFO : PROGRESS: at 13.69% examples, 6094 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:23,233 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:23,270 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:23,279 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:23,295 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:23,301 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:23,341 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:23,347 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:23,356 : INFO : training on 72170 raw words (47379 effective words) took 1.2s, 38622 effective words/s\n",
      "2017-12-13 13:25:23,364 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:23,367 : INFO : Epoch 21\n",
      "2017-12-13 13:25:23,376 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:24,516 : INFO : PROGRESS: at 13.81% examples, 6198 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:24,530 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:24,544 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:24,559 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:24,594 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:24,618 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:24,631 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:24,651 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:24,659 : INFO : training on 72170 raw words (47379 effective words) took 1.2s, 39593 effective words/s\n",
      "2017-12-13 13:25:24,667 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:24,669 : INFO : Epoch 22\n",
      "2017-12-13 13:25:24,682 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:25,806 : INFO : PROGRESS: at 13.81% examples, 5990 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:25,817 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:25,848 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:25,866 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:25,925 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:25,952 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:25,970 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:25,981 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:25,984 : INFO : training on 72170 raw words (47354 effective words) took 1.3s, 37126 effective words/s\n",
      "2017-12-13 13:25:25,989 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:25,993 : INFO : Epoch 23\n",
      "2017-12-13 13:25:26,002 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:27,093 : INFO : PROGRESS: at 13.92% examples, 6101 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:27,173 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:27,190 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:27,214 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:27,216 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:27,231 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:27,256 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:27,268 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:27,275 : INFO : training on 72170 raw words (47326 effective words) took 1.3s, 37854 effective words/s\n",
      "2017-12-13 13:25:27,284 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:27,287 : INFO : Epoch 24\n",
      "2017-12-13 13:25:27,297 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:28,317 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:28,357 : INFO : PROGRESS: at 41.51% examples, 19169 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-13 13:25:28,362 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:28,385 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:28,388 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:28,397 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:28,411 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:28,427 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:28,430 : INFO : training on 72170 raw words (47446 effective words) took 1.1s, 43032 effective words/s\n",
      "2017-12-13 13:25:28,436 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:28,443 : INFO : Epoch 25\n",
      "2017-12-13 13:25:28,459 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:29,528 : INFO : PROGRESS: at 27.70% examples, 12619 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-13 13:25:29,531 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:29,548 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:29,593 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:29,599 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:29,638 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:29,645 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:29,649 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:29,654 : INFO : training on 72170 raw words (47399 effective words) took 1.2s, 40860 effective words/s\n",
      "2017-12-13 13:25:29,656 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:29,663 : INFO : Epoch 26\n",
      "2017-12-13 13:25:29,671 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:30,711 : INFO : PROGRESS: at 13.75% examples, 6495 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:30,721 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:30,746 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:30,782 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:30,797 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:30,808 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:30,822 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:30,831 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:30,833 : INFO : training on 72170 raw words (47403 effective words) took 1.1s, 41965 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-13 13:25:30,837 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:30,840 : INFO : Epoch 27\n",
      "2017-12-13 13:25:30,851 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:31,873 : INFO : PROGRESS: at 13.94% examples, 6528 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:31,887 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:31,895 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:31,900 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:31,911 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:31,918 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:31,926 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:31,951 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:31,953 : INFO : training on 72170 raw words (47457 effective words) took 1.1s, 43590 effective words/s\n",
      "2017-12-13 13:25:31,956 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:31,960 : INFO : Epoch 28\n",
      "2017-12-13 13:25:31,969 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:32,962 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:33,041 : INFO : PROGRESS: at 41.15% examples, 18604 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-13 13:25:33,043 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:33,056 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:33,060 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:33,087 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:33,090 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:33,105 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:33,110 : INFO : training on 72170 raw words (47436 effective words) took 1.1s, 42209 effective words/s\n",
      "2017-12-13 13:25:33,117 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:33,120 : INFO : Epoch 29\n",
      "2017-12-13 13:25:33,133 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:34,192 : INFO : PROGRESS: at 13.93% examples, 6389 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:34,217 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:34,243 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:34,259 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:34,266 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:34,277 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:34,291 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:34,305 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:34,309 : INFO : training on 72170 raw words (47287 effective words) took 1.2s, 40740 effective words/s\n",
      "2017-12-13 13:25:34,313 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:34,317 : INFO : Epoch 30\n",
      "2017-12-13 13:25:34,327 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:35,498 : INFO : PROGRESS: at 13.82% examples, 5673 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:35,530 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:35,558 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:35,588 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:35,606 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:35,631 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:35,649 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:35,669 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:35,671 : INFO : training on 72170 raw words (47435 effective words) took 1.3s, 35717 effective words/s\n",
      "2017-12-13 13:25:35,680 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:35,691 : INFO : Epoch 31\n",
      "2017-12-13 13:25:35,699 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:36,768 : INFO : PROGRESS: at 13.72% examples, 6310 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:36,796 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:36,824 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:36,832 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:36,835 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:36,856 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:36,869 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:36,924 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:36,927 : INFO : training on 72170 raw words (47415 effective words) took 1.2s, 39918 effective words/s\n",
      "2017-12-13 13:25:36,930 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:36,933 : INFO : Epoch 32\n",
      "2017-12-13 13:25:36,943 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:38,129 : INFO : PROGRESS: at 13.87% examples, 6052 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:38,154 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:38,177 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:38,198 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:38,213 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:38,217 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:38,248 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:38,263 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:38,270 : INFO : training on 72170 raw words (47497 effective words) took 1.2s, 38419 effective words/s\n",
      "2017-12-13 13:25:38,272 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:38,275 : INFO : Epoch 33\n",
      "2017-12-13 13:25:38,287 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:39,517 : INFO : PROGRESS: at 13.78% examples, 5543 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:39,536 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:39,540 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:39,552 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:39,563 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:39,618 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:39,627 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:39,637 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:39,639 : INFO : training on 72170 raw words (47294 effective words) took 1.3s, 36040 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-13 13:25:39,644 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:39,646 : INFO : Epoch 34\n",
      "2017-12-13 13:25:39,655 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:40,923 : INFO : PROGRESS: at 13.78% examples, 5758 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:40,974 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:40,978 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:41,031 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:41,069 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:41,073 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:41,089 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:41,100 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:41,109 : INFO : training on 72170 raw words (47349 effective words) took 1.3s, 35602 effective words/s\n",
      "2017-12-13 13:25:41,114 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:41,118 : INFO : Epoch 35\n",
      "2017-12-13 13:25:41,133 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:43,076 : INFO : PROGRESS: at 13.96% examples, 5425 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:43,145 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:43,162 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:43,167 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:43,197 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:43,209 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:43,230 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:43,240 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:43,243 : INFO : training on 72170 raw words (47477 effective words) took 1.4s, 34531 effective words/s\n",
      "2017-12-13 13:25:43,247 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:43,251 : INFO : Epoch 36\n",
      "2017-12-13 13:25:43,261 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:44,306 : INFO : PROGRESS: at 13.89% examples, 6515 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:44,323 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:44,360 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:44,498 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:44,587 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:44,597 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:44,609 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:44,614 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:44,617 : INFO : training on 72170 raw words (47319 effective words) took 1.3s, 35737 effective words/s\n",
      "2017-12-13 13:25:44,620 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:44,625 : INFO : Epoch 37\n",
      "2017-12-13 13:25:44,639 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:45,889 : INFO : PROGRESS: at 13.83% examples, 6073 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:46,103 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:46,116 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:46,125 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:46,134 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:46,156 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:46,181 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:46,186 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:46,189 : INFO : training on 72170 raw words (47382 effective words) took 1.4s, 34533 effective words/s\n",
      "2017-12-13 13:25:46,196 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:46,199 : INFO : Epoch 38\n",
      "2017-12-13 13:25:46,211 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:47,545 : INFO : PROGRESS: at 13.82% examples, 5983 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:47,613 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:47,630 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:47,639 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:47,686 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:47,698 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:47,703 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:47,719 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:47,722 : INFO : training on 72170 raw words (47364 effective words) took 1.3s, 37464 effective words/s\n",
      "2017-12-13 13:25:47,726 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:47,731 : INFO : Epoch 39\n",
      "2017-12-13 13:25:47,744 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:48,932 : INFO : PROGRESS: at 13.61% examples, 6228 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:48,975 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:48,979 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:49,024 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:49,044 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:49,111 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:49,124 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:49,130 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:49,134 : INFO : training on 72170 raw words (47349 effective words) took 1.2s, 37904 effective words/s\n",
      "2017-12-13 13:25:49,143 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:49,147 : INFO : Epoch 40\n",
      "2017-12-13 13:25:49,156 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:50,269 : INFO : PROGRESS: at 27.67% examples, 12923 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-13 13:25:50,272 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:50,307 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:50,317 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:50,349 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:50,358 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:50,363 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:50,370 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:50,372 : INFO : training on 72170 raw words (47434 effective words) took 1.1s, 42290 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-13 13:25:50,376 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:50,380 : INFO : Epoch 41\n",
      "2017-12-13 13:25:50,391 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:51,559 : INFO : PROGRESS: at 27.75% examples, 11887 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-13 13:25:51,562 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:51,597 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:51,629 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:51,637 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:51,648 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:51,657 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:51,677 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:51,680 : INFO : training on 72170 raw words (47571 effective words) took 1.2s, 38661 effective words/s\n",
      "2017-12-13 13:25:51,687 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:51,691 : INFO : Epoch 42\n",
      "2017-12-13 13:25:51,704 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:52,902 : INFO : PROGRESS: at 13.69% examples, 5764 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-13 13:25:52,958 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:52,967 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:52,973 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:52,977 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:52,986 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:52,993 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:53,008 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:53,010 : INFO : training on 72170 raw words (47503 effective words) took 1.2s, 38139 effective words/s\n",
      "2017-12-13 13:25:53,014 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:53,018 : INFO : Epoch 43\n",
      "2017-12-13 13:25:53,027 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:54,057 : INFO : PROGRESS: at 27.97% examples, 13002 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-13 13:25:54,059 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:54,067 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:54,082 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:54,112 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:54,120 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:54,123 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:54,136 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:54,137 : INFO : training on 72170 raw words (47415 effective words) took 1.1s, 43370 effective words/s\n",
      "2017-12-13 13:25:54,140 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:54,144 : INFO : Epoch 44\n",
      "2017-12-13 13:25:54,156 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:55,052 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:55,060 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:55,069 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:55,079 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:55,086 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:55,093 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:55,102 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:55,106 : INFO : training on 72170 raw words (47388 effective words) took 0.9s, 50651 effective words/s\n",
      "2017-12-13 13:25:55,108 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:55,112 : INFO : Epoch 45\n",
      "2017-12-13 13:25:55,122 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:55,948 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:55,957 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:55,963 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:55,965 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:55,977 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:55,980 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:55,999 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:56,000 : INFO : training on 72170 raw words (47550 effective words) took 0.9s, 55123 effective words/s\n",
      "2017-12-13 13:25:56,003 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:56,006 : INFO : Epoch 46\n",
      "2017-12-13 13:25:56,013 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:56,849 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:56,859 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:56,865 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:56,874 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:56,884 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:56,888 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:56,903 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:56,905 : INFO : training on 72170 raw words (47584 effective words) took 0.9s, 54696 effective words/s\n",
      "2017-12-13 13:25:56,907 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:56,911 : INFO : Epoch 47\n",
      "2017-12-13 13:25:56,920 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:57,740 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:57,758 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:57,762 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:57,765 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:57,772 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:57,777 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:57,794 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:57,796 : INFO : training on 72170 raw words (47324 effective words) took 0.9s, 55016 effective words/s\n",
      "2017-12-13 13:25:57,800 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:57,806 : INFO : Epoch 48\n",
      "2017-12-13 13:25:57,812 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:58,691 : INFO : worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-13 13:25:58,697 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:58,701 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:58,710 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:58,718 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:58,723 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:58,746 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:58,749 : INFO : training on 72170 raw words (47278 effective words) took 0.9s, 51648 effective words/s\n",
      "2017-12-13 13:25:58,752 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:58,756 : INFO : Epoch 49\n",
      "2017-12-13 13:25:58,764 : INFO : training model with 7 workers on 2712 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-12-13 13:25:59,605 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-13 13:25:59,621 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-13 13:25:59,629 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-13 13:25:59,636 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-13 13:25:59,646 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-13 13:25:59,649 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-13 13:25:59,664 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-13 13:25:59,666 : INFO : training on 72170 raw words (47400 effective words) took 0.9s, 53881 effective words/s\n",
      "2017-12-13 13:25:59,668 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-13 13:25:59,671 : INFO : saving Doc2Vec object under ./imdb.d2v, separately None\n",
      "2017-12-13 13:25:59,676 : INFO : not storing attribute syn0norm\n",
      "2017-12-13 13:25:59,680 : INFO : not storing attribute cum_table\n",
      "2017-12-13 13:25:59,844 : INFO : saved ./imdb.d2v\n"
     ]
    }
   ],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# shuffle\n",
    "from random import shuffle\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "import _pickle as pickle\n",
    "#import cPickle as pickle   #Note: in python3, _pickle was used instead of cpickle\n",
    "\n",
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "\n",
    "        flipped = {}\n",
    "\n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "\n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(\n",
    "                        utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "\n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences\n",
    "\n",
    "sources = {'test-neg.txt':'TEST_NEG', 'test-pos.txt':'TEST_POS', 'train-neg.txt':'TRAIN_NEG', 'train-pos.txt':'TRAIN_POS', 'train-unsup.txt':'TRAIN_UNS'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)\n",
    "\n",
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model.build_vocab(sentences.to_array())\n",
    "\n",
    "for epoch in range(50):\n",
    "    logger.info('Epoch %d' % epoch)\n",
    "    model.train(sentences.sentences_perm(),\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter,\n",
    "    )\n",
    "\n",
    "model.save('./imdb.d2v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('qihu', 0.7555435299873352),\n",
       " ('load', 0.7273313999176025),\n",
       " ('automakers', 0.7238010168075562),\n",
       " ('spot', 0.7107664346694946),\n",
       " ('domestic', 0.7089092135429382),\n",
       " ('loaded', 0.6962469816207886),\n",
       " ('terribly', 0.6833244562149048),\n",
       " ('biotechs', 0.6804578304290771),\n",
       " ('some', 0.6749091148376465),\n",
       " ('note', 0.6718221306800842)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('totally', 0.9983320236206055),\n",
       " ('absolute', 0.995948851108551),\n",
       " ('pure', 0.9928412437438965),\n",
       " ('garbage', 0.9846278429031372),\n",
       " ('stores', 0.9469236731529236),\n",
       " ('empty', 0.9068784713745117),\n",
       " ('merchant', 0.8186815977096558),\n",
       " ('ross', 0.7849423289299011),\n",
       " ('usio', 0.7547214031219482),\n",
       " ('rbc', 0.7533202767372131)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('mispriced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('retest', 0.8763420581817627),\n",
       " ('helping', 0.8424099683761597),\n",
       " ('itting', 0.8358184099197388),\n",
       " ('broken', 0.771192729473114),\n",
       " ('line', 0.7534346580505371),\n",
       " ('average', 0.7364001274108887),\n",
       " ('resistance', 0.7189692854881287),\n",
       " ('trend', 0.7136515378952026),\n",
       " ('beautiful', 0.7084102630615234),\n",
       " ('bidders', 0.7053914070129395)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('downtrend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('awesomely', 0.8679783940315247),\n",
       " ('ief', 0.8635176420211792),\n",
       " ('tlt', 0.8232649564743042),\n",
       " ('crossovers', 0.8114673495292664),\n",
       " ('decreased', 0.797480583190918),\n",
       " ('abc', 0.7540429830551147),\n",
       " ('conversations', 0.7478561401367188),\n",
       " ('ewz', 0.7472898960113525),\n",
       " ('twitter', 0.729065477848053),\n",
       " ('dip', 0.7279219627380371)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('bullish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('steep', 0.8865960240364075),\n",
       " ('view', 0.8646053671836853),\n",
       " ('posts', 0.8605137467384338),\n",
       " ('rewards', 0.8602394461631775),\n",
       " ('patient', 0.8539063930511475),\n",
       " ('bright', 0.8084157705307007),\n",
       " ('wall', 0.8080934286117554),\n",
       " ('tops', 0.8000960350036621),\n",
       " ('percent', 0.7816009521484375),\n",
       " ('global', 0.7796220183372498)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('increase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((1700,100))\n",
    "train_labels = numpy.zeros((1700))\n",
    "for i in range(1119):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    train_arrays[i] = model.docvecs[prefix_train_pos]\n",
    "    train_labels[i] = 1\n",
    "for i in range(581):\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays[i+1119] = model.docvecs[prefix_train_neg]\n",
    "    train_labels[i+1119] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0177709  -0.22923303 -0.0342895  ...,  0.07055531  0.0153681\n",
      "  -0.10305636]\n",
      " [ 0.06589964  0.17978436  0.21489561 ..., -0.29530606 -0.00678638\n",
      "  -0.60204124]\n",
      " [ 0.00429939 -0.08192944  0.47957188 ..., -0.10725647  0.07766831\n",
      "  -0.3334069 ]\n",
      " ..., \n",
      " [-0.0291836   0.03448483  0.08596164 ..., -0.06617699  0.09165017\n",
      "  -0.10532157]\n",
      " [-0.11976746  0.44390383  0.34118399 ..., -0.64246607 -0.22330751\n",
      "  -0.11591343]\n",
      " [ 0.05145888  0.00953461  0.03342689 ..., -0.12771818 -0.00633792\n",
      "  -0.06922003]]\n"
     ]
    }
   ],
   "source": [
    "print(train_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05145888,  0.00953461,  0.03342689,  0.18303509,  0.12342305,\n",
       "       -0.01732425, -0.01066432, -0.07760131,  0.1367396 , -0.09315624,\n",
       "       -0.07442326, -0.13977994, -0.13990229,  0.09215616,  0.04037021,\n",
       "       -0.03223286,  0.06288718, -0.07213554, -0.04514331,  0.00884892,\n",
       "       -0.03508247,  0.06330335, -0.03663046, -0.02831447,  0.03311158,\n",
       "       -0.06097967,  0.09175718, -0.00285468, -0.09703495, -0.06782337,\n",
       "        0.00418406,  0.06110639,  0.23725958, -0.00133014, -0.00380763,\n",
       "        0.00274535, -0.02215949, -0.06008369, -0.05410678, -0.04985861,\n",
       "        0.22829747, -0.13805626, -0.10648111, -0.15145004,  0.06456409,\n",
       "        0.01236629,  0.11522083, -0.15615945, -0.12437502,  0.01580986,\n",
       "        0.07552686,  0.18161389, -0.02375856,  0.02106199, -0.00251829,\n",
       "       -0.04400126,  0.09193997, -0.06060835,  0.0235026 , -0.04354072,\n",
       "       -0.02132745, -0.05010912, -0.06865673,  0.15085252, -0.06799036,\n",
       "       -0.05719998, -0.16104843, -0.18194994,  0.00066329,  0.10852819,\n",
       "       -0.02208314,  0.02209603, -0.09169088,  0.12743101,  0.02027584,\n",
       "       -0.08310311, -0.11846465,  0.10497718,  0.01139878,  0.0431264 ,\n",
       "       -0.08252512,  0.09650236,  0.03476907, -0.02524382, -0.08259674,\n",
       "        0.03268375,  0.13827878,  0.04456   , -0.02733547,  0.14988771,\n",
       "        0.01288288, -0.14955975, -0.03955226,  0.15657495,  0.04782077,\n",
       "        0.05305295, -0.17875154, -0.12771818, -0.00633792, -0.06922003], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs['TRAIN_NEG_580']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[1118])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays = numpy.zeros((794,100))\n",
    "test_labels = numpy.zeros((794))\n",
    "for i in range(537):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    test_arrays[i] = model.docvecs[prefix_test_pos]\n",
    "    test_labels[i] = 1\n",
    "for i in range(257):\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    test_arrays[i+537] = model.docvecs[prefix_test_neg]\n",
    "    test_labels[i+537] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71410579345088165"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=42, splitter='best')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = tree_reg.predict(train_arrays)\n",
    "tree_mse = mean_squared_error(train_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71410579345088165"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous-multioutput and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-5f590d80a272>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m   1259\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'precision'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1262\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous-multioutput and binary targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(test_arrays, test_labels, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.72857143,  0.76470588,  0.7027027 ,  0.72881356,  0.72307692,\n",
       "        0.72058824,  0.70967742,  0.81818182,  0.765625  ,  0.67164179])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "precision_list = cross_val_score(classifier, test_arrays, test_labels, cv=10, scoring='precision')\n",
    "precision_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73335847599015813"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "for item in precision_list:\n",
    "    sum += item\n",
    "precision = sum/len(precision_list)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.94444444,  0.96296296,  0.96296296,  0.7962963 ,  0.87037037,\n",
       "        0.90740741,  0.81481481,  0.8490566 ,  0.9245283 ,  0.8490566 ])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_list = cross_val_score(classifier, test_arrays, test_labels, cv=10, scoring='recall')\n",
    "recall_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88819007686932228"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_recall = 0\n",
    "for item in recall_list:\n",
    "    sum_recall += item\n",
    "recall = sum_recall/len(recall_list)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80338232242733598"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = 2*(precision * recall)/(precision + recall)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
